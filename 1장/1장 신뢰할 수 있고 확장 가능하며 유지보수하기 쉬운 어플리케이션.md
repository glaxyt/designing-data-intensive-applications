## 오늘날 많은 어플리케이션은 계산 중심과 다르게 데이터 중심적이다.

이런 애플리케이션은 데이터의 양, 데이터 복잡도, 데이터의 변화 속도에 의한 제한을 받는다.

일반적으로 데이터 중심 어플리케이션언 다음을 필요로 한다

- 구동 애플리케이션이나 다른 애플리케이션에서 나중에 다시 데이터를 찾을 수 있게 데이터를 저장(데이터베이스)
- 읽기 속도 향상을 위해 값비싼 수행 결과를 기억(캐시)
- 사용자가 키워드로 데이터를 검색하거나 다양한 방법으로 필터링할 수 있게 제공(검색 색인)
- 비동기 처리를 위해 다른 프로세스 메세지 보내기(스트림 처리)
- 주기적으로 대량의 누적된 데이터를 분석(일괄 처리)

## 데이터 시스템에 대한 생각

- work → (효율적으로 수행가능한) task로 분할
- 다양한 도구들은 애플리케이션 코드를 이용해 서로 연결한다.

<img width="1068" alt="image" src="https://github.com/user-attachments/assets/40317b9d-ba17-4fc0-8051-ec6563ffc77b" />

- API(Application Programming Interface)는 클라이언트가 모르게 구현 세부 사항을 숨긴다.

# **Reliability(신뢰성)**

- **신뢰성** : 무언가 잘못되더라도 지속적으로 올바르게 동작함
- **결함 (fault)** : 잘못될 수 있는 일
  - **결함** : 사양에서 벗어난 시스템의 한 구성 요소
  - **장애 (failure)** : 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우
- **내결함성(fault-tolerant) or 탄력성(resilient)** : 결함을 예측하고 대처할 수 있는 능력
- **결함**은 **장애**와 동일하지 않다.
  - 결함은 사양에서 벗어난 시스템의 한 구성 요소로 정의되지만, 장애는 사용자에게 필요한 서비스를 제공하지 못하고 **시스템 전체**가 멈춘 경우를 뜻한다.
- 실제로 많은 중대한 **버그**는 미흡한 오류 처리에 기인한다.
  - **카오스 몽키 (Chaos Monkey)** : 서비스를 공급하는 인스턴스에 일부러 랜덤으로 장애를 일으킨다. 이를 통해 미래에 발생할 장애에 대해 빠르게 대응할 수 있는 시스템을 구축함
  - **인스턴스 (instance)** : 객체 (object)가 값을 갖을 경우를 말함

## **Hardware Faults(하드웨어 결함)**

- 시스템 장애율을 줄이기 위한 첫 번째 대응으로 각 하드웨어 구성 요소에 **중복 (redundancy)**을 추가하는 것이다.
- 하지만 데이터 양과 애플리케이션의 계산 요구가 늘어나면서 더 많은 애플리케이션이 많은 수의 장비를 사용하게 되었고 이에 하두에어 결함율도 증가했다.
- AWS 플랫폼은 단일 장비 신뢰성보다 유연성(flexibility)과 탄력성(elasticity)을 우선적으로 처리하게끔 설계됐다.
- 따라서 소프트웨어 내결함성 기술을 사용하거나 하드웨어 중복성을 추가해 전체 장비의 손실을 견딜 수 있는 시스템으로 점점 옮겨가고 있다.

## Software Errors(소프트웨어 오류)

- 소프트웨어의 체계적 오류 문제는 신속한 해결 방법이 없다.
- 시스템 설계를 하고 운영하면서 고려해야하는 점은 아래와 같다.
  - 시스템 가정과 상호작용에 대해 주의 깊게 생각하기
  - 빈틈없는 테스트
  - 프로세스 격리
  - 죽은 프로세스의 재시작 허용
  - 프로덕션 환경에서 시스템의 동작 측정
  - 모니터링
  - 분석하기
- 따라서 소프트웨어 오류는 발생 시 신속한 해결 방법이 없으니 위의 경우를 고려해서 설계하고 관리해야한다.

## **Human Errors(인적 오류)**

- 대규모 인터넷 서비스에 대한 연구에 따르면 대부분의 오류는 **운영자의 설정 오류**가 중단의 주요 원인으로 밝혀졌다.
- 사람은 불완전하다. 그럼에도 인적 오류를 줄이는 방법은 아래와 같다.
  - 추상화, API, 관리 인터페이스를 활용하여 오류의 가능성을 최소화하는 시스템을 설계하는 것
    - 다만 과도한 인터페이스 제한은 코드 생산성을 감소시킨다.
  - 사람이 가장 많이 실수하는 장소에서 사람의 실수로 장애가 발생할 수 있는 부분을 분리한다.
    - **샌드박스 (sandbox)** : 실제 데이터를 사용해 안전하게 살펴보고 실험할 수 있지만 실제 사용자에게는 영향이 없는 비 프로덕션
  - 장애 발생의 영향을 최소화하기 위해 인적 오류를 빠르고 쉽게 복구할 수 있게 한다.
  - 단위 테스트부터 전체 시스템 통합 테스트와 수동 테스트까지 모든 수준에서 철저하게 테스트한다.
  - 성능 지표와 오류율 같은 상세하고 명확한 모니터링 대책을 마련한다.

# **Scalability(확장성)**

- 시스템이 안정적으로 운영된다고 미래에도 안정적으로 운영된다는 보장은 없다.
  - 이 때 성능 저하를 유발하는 흔한 이유 중 하나는 부하 증가다.
- 우리가 고민해야하는 것은 아래와 같다.
  - 시스템이 특정 방식으로 커지면, 이에 대처하기 위한 선택은 무엇인가?
  - 추가 부하를 다루기 위해 계산 자원을 어떻게 투입할까?
- 모든 것은 부하와 연관되어있다. 따라서 확장성은 부하와 연관지어서 설명할 수 있어야 한다.
- **확장성 (scalability)** 은 증가한 부하에 대처하는 시스템 능력을 설명하는 데 사용하는 용어
  - 부하가 증가해도 좋은 성능을 유지하기 위한 전략

## **Describing Load(부하 기술하기)**

- 부하는 부하 매개변수(Load Parameter)로 설명할 수 있다.
  - 여기서 부하 매개변수는 시스템의 목적성에 따라 구분될 수 있으며, 더 나아가 비즈니스 로직과 밀접하게 관련되어있다.
  - Ex) 웹 서버의 초당 요청 수, 데이터베이스의 읽기 대 쓰기 비율, 대화방의 동시 활성 사용자, 캐시 적중률 등

### 🐦 트위터의 팬 아웃(Fan-out) 예시

- 트위터에서 한 사용자가 트윗을 작성할 때, 그 트윗은 **팔로워 수만큼의 타임라인에 복제**된다.
- 예를 들어, **10만 명의 팔로워가 있는 유명인이 트윗을 올리면, 10만 번의 쓰기 작업이 발생**한다.

이 때 시스템에 영향을 주는 주요 **부하 매개변수**는 다음과 같다:

| 부하 매개변수           | 설명                                                                            |
| ----------------------- | ------------------------------------------------------------------------------- |
| 사용자당 평균 팔로워 수 | 팬 아웃 횟수를 결정짓는 핵심 지표. (팔로워 수 1명 vs 100만 명은 전혀 다른 부하) |
| 팬 아웃 시 복제 대상 수 | 한 트윗이 타임라인에 몇 번 복제되는가?                                          |
| 트윗 작성 빈도          | 전체 팬 아웃 횟수를 결정하는 주요 요소                                          |
| 타임라인 조회 빈도      | 팬 아웃 전략이 Pull 방식일 경우 여기에 의존                                     |
| 사용자 군 분포          | 상위 1% 유저가 전체 부하의 대부분을 발생시키는 ‘헤비테일’ 구조                  |

### 팬 아웃은 설계 전략을 바꾼다

- 이러한 특성 때문에 트위터는 팬 아웃을 **쓰기 시점에 할 것인가(Fan-out on write)**, 아니면 **읽기 시점에 할 것인가(Fan-out on read**)라는 전략적 선택에 직면하게 된다.
  - _쓰기 팬 아웃:_ 트윗을 쓸 때 모든 팔로워의 타임라인에 즉시 반영 (쓰기 부하 ↑, 읽기 부하 ↓)
  - _읽기 팬 아웃:_ 타임라인을 조회할 때마다 관련 트윗을 조합 (쓰기 부하 ↓, 읽기 부하 ↑)

### 결론

- "초당 요청 수" 같은 일반적인 지표만으로는 시스템 부하를 제대로 설명할 수 없다.
- 트위터처럼 **관계 기반 콘텐츠 전파**가 중요한 서비스에서는, 도메인 특화된 지표(팔로워 수, 팬 아웃 깊이 등)가 진짜 부하를 설명해준다.
- 부하를 정확히 기술해야만 **올바른 확장 전략, 캐싱 구조, DB 파티셔닝**이 가능하다.

## **Describing Performance(성능 기술하기)**

- 하둡같은 일괄 처리 시스템은 보통 **처리량 (throughput)**에 관심을 가진다.
  - 처리량은 초당 처리할 수 있는 레코드 수나 일정 크기의 데이터 집합으로 작업을 수행할 때 걸리는 전체 시간
- 온라인 시스템에서 더 중요한 사항은 서비스 **응답 시간 (response time)**이다.
  - 응답 시간은 클라이언트가 요청을 보내고 응답을 받는 사이의 시간

### 응답 시간 vs 지연 시간

- **응답 시간 (response time)** : 클라이언트 관점에서 본 시간. 요청을 처리하는 실제 시간(서비스 시간) 외에도 네트워크 지연과 큐 지연도 포함된다.
- **지연 시간 (latency)** : 요청이 처리되길 기다리는 시간으로, 서비스를 기다리며 휴지(latent) 상태인 시간

| 용어                          | 정의                                                  | 포함 범위                                   |
| ----------------------------- | ----------------------------------------------------- | ------------------------------------------- |
| **응답 시간** (Response Time) | 요청을 보낸 시점부터 응답을 받는 시점까지의 전체 시간 | **지연 시간 + 서비스 시간 + 네트워크 시간** |
| **지연 시간** (Latency)       | 요청이 대기열 등에서 **처리되기를 기다리는 시간**     | **큐에 쌓여있는 시간** 등                   |

### 응답 시간을 올바르게 분석하는 방법

- 클라이언트가 몇 번이고 반복해서 동일한 요청을 하더라도 매번 응답 시간이 다르기 때문에 응답 시간을 **분포**로 생각해야 한다.
- 백분위는 **서비스 수준 목표 (SLO; service level objective)** 와 **서비스 수준 협약서 (SLA; service level agreement)** 에 자주 사용한다.
- 평균은 사용자의 실제 지연 경험을 반영하지 못한다. → 지연 시간이 발생하는 응답 시간의 데이터인 특이 값은 희석되고만다.
  - 평균이 아닌 분포를 통한 중앙값으로 분석해야하는 것이 올바르다.

<img width="834" alt="image" src="https://github.com/user-attachments/assets/9e226dc0-c5c9-4ec4-8405-e8e6b3db961e" />

- 487ms의 응답 시간은 지연을 유발한다. 그러나 산술 평균에 의한 계산 값에 의하면 시스템 관리자는 사용자가 지연을 경험했다고 인식하지 못한다.

## **Approaches for Coping with Load(부하 대응 접근 방식)**

- **수직 확장 (vertical scaling)** : 좀 더 강력한 장비로 이동
- **수평 확장 (horizontal scaling)** : 다수의 낮은 사양 장비에 부하를 분산
- **비공유 아키텍처 (sharing-nothing architecture)** : 다수의 장비에 부하를 분산하는 아키텍처
- 일부 시스템은 **탄력적**이다.
  - **탄력적 (elastic)** : 부하 증가를 감지하면 컴퓨팅 자원을 자동으로 추가할 수 있다.
  - 탄력적인 시스템은 운영상 예상치 못한 일이 수동 확장 시스템보다 많이 발생한다. 따라서 시스템에 따라 탄력적인 시스템 혹은 수동적인 확장 시스템으로 운영할지 결정해야한다.
- 특정 애플리케이션에 적합한 확장성을 갖춘 아키텍처는 주요 동작이 무엇이고 잘 하지 않는 동작이 무엇인지에 대한 가정을 바탕으로 구축한다.

# **Maintainability(유지보수성)**

- **유지보수**에는 버그 수정, 시스템 운영 유지, 장애 조사, 새로운 플랫폼 적응, 새 사용 사례를 위한 변경, 기술 채무 상환, 새로운 기능 추가 등이 있다.
- 소프트웨어 시스템 설계의 3가지 원칙
  1. **운용성 (operability)** : 운영팀이 시스템을 원할하게 운영할 수 있게 쉽게 만들어야 한다.
  2. **단순성 (simplicity)** : 시스템에서 복잡도를 최대한 제거해 새로운 엔지니어가 시스템을 이해하기 쉽게 만든다.
  3. **발전성 (evolvability)** : 엔지니어가 이후에 시스템을 쉽게 변경할 수 있게 한다. 이 속성은 유연성(extensibility), 수정 가능성(modifiability), 적응성(plasticity)로 알려져 있다.

## **Operability: Making Life Easy for Operations (운용성: 운영의 편리함 만들기)**

좋은 시스템은 단순히 잘 작동하는 것뿐 아니라, **운영 과정 전반이 예측 가능하고 효율적이어야 한다**. 운영자는 시스템을 매일 돌보며 장애에 대응하고 배포를 반복하며, 보안과 안정성을 유지해야 한다. 따라서 시스템은 운영자에게 **명확한 관측 수단, 자동화된 수단, 장애 대응력**을 제공해야 한다.

### 운영성을 고려한 시스템 설계의 핵심 요소

- **관측 가능성 (Observability)**
  시스템 내부의 상태(예: 지연, 오류, 부하)를 외부에서 파악할 수 있어야 한다. 메트릭, 로그, 분산 트레이싱은 핵심 도구다.
- **자동화 가능성 (Automatability)**
  반복적인 운영 태스크(배포, 롤백, 스케일링, 설정 반영)는 수작업이 아닌 코드/스크립트로 자동화되어야 한다.
- **장애 복구 능력 (Resilience & Recoverability)**
  문제가 발생했을 때 빠르게 탐지하고, 원인을 추적하며, 안전하게 복구할 수 있어야 한다. 알림 시스템과 로그 상관 분석 도구는 매우 중요하다.
- **운영 지식의 전파 가능성 (Operability over time)**
  개인이 퇴사하거나 팀이 바뀌어도 운영 노하우가 사라지지 않도록, 운영 절차와 설정이 명문화되고 재현 가능해야 한다.
- **안정된 배포와 환경 구성**
  - 환경 간 차이를 줄이기 위한 Infrastructure as Code
  - Canary/Blue-Green Deployment 전략 도입
  - 설정 변경이 무중단으로 반영되도록 설계

### 운영팀이 반복적으로 수행해야 하는 업무 예시

- 시스템 상태 실시간 모니터링 및 알림
- 서비스 성능 저하 원인 분석
- 보안 업데이트 및 취약점 대응
- 시스템 간 의존성 영향도 분석
- 장애 예측을 위한 로그 및 메트릭 분석
- 신규 배포/롤백을 위한 절차 문서화
- 환경 이전(예: 클라우드 이관) 수행
- 설정 변경으로 인한 보안 위험 점검
- 교육 및 핸드오버 문서 관리

> 좋은 운용성은 운영팀이 단순 반복 작업이 아닌, 서비스 품질 개선과 전략적 기술 의사결정에 집중할 수 있도록 지원하는 구조를 의미한다.

## **Simplicity: Managing Complexity (단순성: 복잡도 관리)**

모든 시스템은 시간이 지날수록 복잡해진다. 복잡도는 설계, 구현, 유지보수, 팀 커뮤니케이션 전반에 영향을 미친다. 시스템이 복잡해질수록 문제 해결에 드는 시간은 증가하고, 버그는 사라지지 않으며, 작은 변경도 큰 리스크를 동반한다.

### 복잡도의 주요 증상

- 상태 공간의 폭발적 증가
- 모듈 간 강한 결합과 순환 참조
- 불명확하고 중복된 용어/이름
- 구조를 이해하지 못한 채 이루어진 빠른 패치
- 예외적 케이스를 위한 비직관적인 코드 추가

### 우발적 복잡도 (Accidental Complexity)

- 문제 자체의 본질이 아닌, **잘못된 도구 선택, 부적절한 설계, 나쁜 추상화**로 인해 생겨나는 복잡도
- 예시: 과도한 마이크로서비스 분리, 의미 없는 인터페이스 계층 남발, 비표준 상태 관리 방식

### 복잡도를 줄이기 위한 접근

- **좋은 추상화**: 핵심 개념에 집중하고, 구현 세부사항은 감추되 과하게 일반화하지 않는다.
- **일관성 있는 설계**: 용어, 상태 전이, 흐름에 일관된 패턴을 적용한다.
- **강한 캡슐화와 모듈화**: 내부 구현이 외부에 노출되지 않도록 하고, 책임 단위를 명확히 한다.
- **구성 요소 간 약한 결합**: 테스트와 교체가 쉬운 구조를 만든다.
- **적절한 도구 사용**: 복잡도를 오히려 증가시키는 도구는 과감히 배제한다.

> 단순성은 설계의 출발점이 아니라, 정제되고 통제된 구조를 만들기 위한 꾸준한 선택의 결과다.

## **Evolvability: Making Change Easy (발전성: 변화를 쉽게 만들기)**

시스템은 항상 변한다. 사용자는 새로운 기능을 요구하고, 비즈니스 우선순위는 바뀌며, 기술 환경은 지속적으로 진화한다. 따라서 시스템은 **변화에 유연하게 대응할 수 있어야** 하며, 그것이 바로 발전성이다.

### 변화의 주요 원인

- 비즈니스 정책 변경 (예: 요금제, 이벤트)
- 신규 플랫폼 대응 (예: 모바일 앱, IoT, 클라우드)
- 법적/규제 환경 변화 (예: GDPR, 전자금융법)
- 트래픽 증가에 따른 구조적 확장
- 내부 코드 개선, 기술 부채 해소
- 예기치 않은 사용 패턴 등장

### 발전성을 위한 설계 전략

- **작은 단위로 잘게 나눠진 모듈**
  독립적으로 배포되고 수정될 수 있어야 함
- **명확한 도메인 모델 기반 설계**
  변경이 비즈니스 요구사항과 정렬될 수 있도록 의미 있는 모델 구조가 필요
- **테스트 가능성**
  시스템의 진화를 안전하게 하기 위해 자동화된 테스트, 회귀 테스트 기반 확보
- **타입 안정성과 정적 분석의 활용**
  코드 수준의 변경이 어디에 영향을 미치는지 컴파일 타임에 파악 가능해야 함
- **기술 선택의 유연성 확보**
  특정 프레임워크나 라이브러리에 종속되지 않는 아키텍처

### 애자일과의 연관

- 애자일 개발은 발전성을 극대화하기 위한 실행 체계다.
- 빠른 피드백 주기, 반복적 개선, 사용자 중심 기능 설계는 발전성 높은 시스템을 가능하게 한다.

> 발전성은 단기 효율이 아닌, 장기 생존성의 조건이다.

### 참고자료

https://m.yes24.com/goods/detail/59566585

https://ingu627.github.io/diapp/design1/
